import os
import sys

# map repository path inside docker
sys.path.insert(0, '/home/workspace/')
os.environ['PYSPARK_SUBMIT_ARGS'] = """
--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.1  pyspark-shell
"""

from pyspark.conf import SparkConf
from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, to_json, \
    col, unbase64, base64, split, expr
from pyspark.sql.types import StructField, StructType, \
    StringType, BooleanType, ArrayType, DateType, FloatType

from spark_streaming_etl_template.src.constants import BROKER_URL


def process_redis_server(spark, broker_url, topic_input="redis-server"):
    """process in-memory recorded streams

    Args:
        spark(SparkSession)
        broker_url(str)
        topic_input(str)

    Returns:
        DataFrame
    """
    # get streaming data
    df_redis_server = spark \
        .readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", broker_url) \
        .option("subscribe", topic_input) \
        .option("startingOffsets", "earliest") \
        .load()
    # parse raw database entry
    df_redis_server = df_redis_server.selectExpr(
        "cast(value as string) value"
    )
    redisMessageSchema = StructType(
        [
            StructField("key", StringType()),
            StructField("value", StringType()),
            StructField("ch", StringType()),
            StructField("expiredType", StringType()),
            StructField("expiredValue", StringType()),
            StructField("existType", StringType()),
            StructField("incr", BooleanType()),
            StructField("zsetEntries", ArrayType(
                StructType([
                    StructField("element", StringType()),
                    StructField("score", StringType())
                ])
            ))
        ]
    )
    df_redis_server.withColumn("value", from_json("value", redisMessageSchema)) \
        .select(col("value.*")) \
        .createOrReplaceTempView("RedisSortedSet")
    # extract and decode user messages from redis database
    RedisSortedSet = spark.sql("""
       select zsetEntries[0].element as encodedCustomer 
       from RedisSortedSet 
       where key is not null
    """)
    RedisSortedSet = RedisSortedSet.withColumn(
        "encodedCustomer",
        unbase64(RedisSortedSet.encodedCustomer).cast("string"))

    # parse and process user messages
    customerMessage = StructType(
        [
            StructField("customerName", StringType()),
            StructField("email", StringType()),
            StructField("phone", StringType()),
            StructField("birthDay", DateType())
        ]
    )
    RedisSortedSet \
        .withColumn("encodedCustomer",
                    from_json("encodedCustomer", customerMessage)) \
        .select(col("encodedCustomer.*")) \
        .createOrReplaceTempView("CustomerRecords")
    emailAndBirthDayStreamingDF = spark.sql("""
        select * 
        from CustomerRecords 
        where birthDay is not null
        and email is not null
    """)
    # select birth year and email
    emailAndBirthYearStreamingDF = emailAndBirthDayStreamingDF.select(
        "email",
        split(emailAndBirthDayStreamingDF.birthDay, "-").
            getItem(0).alias("birthYear")
    )
    return emailAndBirthYearStreamingDF


def process_customer_events(spark, broker_url, topic_input="stedi-events"):
    """process streams generated by customers

    Args:
        spark(SparkSession)
        broker_url(str)
        topic_input(str)

    Returns:
        DataFrame
    """
    # select customer events
    df_stedi_events = spark \
        .readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", broker_url) \
        .option("subscribe", topic_input) \
        .option("startingOffsets", "earliest") \
        .load()
    df_stedi_events = df_stedi_events.selectExpr("cast(value as string) value")
    # parse events
    stediEvent = StructType(
        [
            StructField("customer", StringType()),
            StructField("score", FloatType()),
            StructField("riskDate", DateType())
        ]
    )
    df_stedi_events.withColumn("value", from_json("value", stediEvent)) \
        .select(col("value.*")) \
        .createOrReplaceTempView("CustomerRisk")
    # select output fields
    customerRiskStreamingDF = spark.sql("""
        select customer, score 
        from CustomerRisk
    """)
    return customerRiskStreamingDF


def run(
        broker_url=BROKER_URL,
        topic_redis="redis-events",
        topic_customer="customer-events",
        topic_output="intelligence-board",
        output_source="kafka",
        timeout=60
):
    """process and merge multiple streams
    Args:
        broker_url(str): url of a kafka broker
        topic_redis(str): name of the topic with the in-memory stored data
        topic_customer(str): name of the topic with customer data
        topic_output(str): name of the topic with the processed stream
        output_source(str): name of the output source
                            "kafka": print the stream in the topic_output
                            "console": print the stream as a log
        timeout(int): seconds to wait before closing the stream

    to deploy this script in a standalone containerised cluster run:
    ```
        docker exec -it spark-streaming-etl-template \
        /opt/bitnami/spark/bin/spark-submit \
        --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0 \
        /home/workspace/spark_streaming_etl_template/scripts/stream_processing.py \
        | tee ../../spark/logs/stream_processing.log
    ```
    """
    # create spark application
    conf = SparkConf().setMaster("spark://localhost:7077") \
        .setAppName('consume-process-produce')
    spark = SparkSession.builder.config(conf=SparkConf()).getOrCreate()
    sc = spark.sparkContext.setLogLevel('WARN')

    # process streams
    customerriskstreamingdf = process_redis_server(
        spark,
        broker_url,
        topic_redis
    )
    emailAndBirthYearStreamingDF = process_customer_events(
        spark,
        broker_url,
        topic_customer
    )
    # merge the stream
    df_merge = customerriskstreamingdf. \
        join(emailAndBirthYearStreamingDF, expr("""
        email = customer
        """))

    # output processed batch
    if output_source == "kafka":
        stream = df_merge \
            .selectExpr("cast(customer as string) as key",
                        "to_json(struct(*)) as value") \
            .writeStream.format("kafka") \
            .option("kafka.bootstrap.servers", "localhost:9092") \
            .option("topic", topic_output) \
            .option("checkpointLocation", "/tmp/kafka_checkpoint_") \
            .start()
        stream.awaitTermination(timeout)
        stream.stop()

    elif output_source == "console":
        stream = df_merge \
            .selectExpr("cast(customer as string) as key",
                        "to_json(struct(*)) as value") \
            .writeStream.format("console") \
            .start()
        stream.awaitTermination(timeout)
        stream.stop()

    # # check as pandas dataframes to debug
    # df_customerriskstreamingdf = sample_data_stream(
    #     spark, customerriskstreamingdf)
    # df_emailAndBirthYearStreamingDF = sample_data_stream(
    #     spark, emailAndBirthYearStreamingDF)


if __name__ == "__main__":
    run()
